{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplitÂ¶\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.datetime.now().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = os.path.join(config.path_data, '')\n",
    "df = pd.read_pickle(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the data is already sorted\n",
    "train_size = 195 # Or better define test size, 60 days\n",
    "train_df = train_and_test[0:train_size]\n",
    "test_df = train_and_test[train_size:]\n",
    "# Or so\n",
    "#train_X, test_X = np.split(train_and_test_X, [int(0.60 *len(train_and_test_X))])\n",
    "#train_y, test_y = np.split(train_and_test_y, [int(0.60 *len(train_and_test_y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have an unbalanced set -- do we need resampling?\n",
    "if DOWNSAMPLING_RATIO:\n",
    "    samplerates = [1, 2, 3]\n",
    "\n",
    "    np.random.seed(29)\n",
    "\n",
    "    indices = []\n",
    "\n",
    "    idx_jam = train_df[train_df.jam == 1].index.values\n",
    "    idx_no_jam = train_df[train_df.jam == 0].index.values\n",
    "\n",
    "    for samplerate in samplerates:\n",
    "        no_jam_to_draw =  len(idx_jam) * samplerate\n",
    "        drawn_idx_no_jam = np.random.choice(\n",
    "            idx_no_jam, size=no_jam_to_draw\n",
    "        ).tolist()\n",
    "\n",
    "        all_idx = idx_jam.tolist() + drawn_idx_no_jam\n",
    "\n",
    "        indices.append(all_idx)\n",
    "\n",
    "train_df = train_df.iloc[indices[DOWNSAMPLING_RATIO - 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target\n",
    "train_y = train_df[response].values\n",
    "train_X = train_df.drop(response, axis=1)\n",
    "\n",
    "test_y = test_df[response].values\n",
    "test_X = test_df.drop(response, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "i = 1\n",
    "for importance, feature in sorted(zip(rf.feature_importances_, X_train.columns), reverse=True):\n",
    "    print(i, '-', round(importance, 3), '\\t', feature)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_RF = RandomForestClassifier() # or GradientBoostingClassifier, or XGBoostClassifier\n",
    "\n",
    "search_grid_RF = {'bootstrap': [True],\n",
    "                  'max_depth': [4, 5, 6, 7],\n",
    "                  'max_features': [3, 4, 5, 6, 7],\n",
    "                  'n_estimators': [100, 200],\n",
    "                  'min_samples_leaf': [10, 25, 50],\n",
    "                  'min_samples_split': [25, 50],\n",
    "                  'random_state': [29]\n",
    "                  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention: DO NOT USE REGULAR CV WITH TIME DATA!!\n",
    "param_search_RF = GridSearchCV(estimator=mod_RF, \n",
    "                               param_grid=search_grid_RF, \n",
    "                               scoring=metrics.make_scorer(metrics.roc_auc_score),\n",
    "                               cv=3, \n",
    "                               n_jobs=-1, \n",
    "                               verbose=2, return_train_score=True,\n",
    "                               iid=True)\n",
    "\n",
    "param_search_RF.fit(X_train, y_train)        \n",
    "\n",
    "print(param_search_RF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_opt_RF = RandomForestClassifier(**param_search_RF.best_params_)\n",
    "mod_opt_RF = mod_opt_RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for accuracy calculation\n",
    "threshold_RF = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of scores by class\n",
    "score_train_RF = mod_opt_RF.predict_proba(train_X)[:, 1]\n",
    "score_test_RF = mod_opt_RF.predict_proba(test_X)[:, 1]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "(pd.DataFrame({'y': train_y, 'Training score': score_train_RF})\n",
    " .boxplot(column='Training score', by='y',\n",
    "          showfliers=True, ax=ax[0]))\n",
    "plt.title('Training score')\n",
    "\n",
    "(pd.DataFrame({'y': test_y, 'Test score': score_test_RF})\n",
    " .boxplot(column='Test score', by='y', showfliers=True, ax=ax[1]))\n",
    "plt.title('Test score')\n",
    "\n",
    "plt.suptitle('Distribution of scores by class')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve.\n",
    "fpr_train_RF, tpr_train_RF, thresholds_train_RF = \\\n",
    "    metrics.roc_curve(y_train, score_train_RF, pos_label=1)\n",
    "\n",
    "fpr_test_RF, tpr_test_RF, thresholds_test_RF = \\\n",
    "    metrics.roc_curve(y_test, score_test_RF, pos_label=1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_train_RF, tpr_train_RF, label='training data')\n",
    "plt.plot(fpr_test_RF, tpr_test_RF, label='test data')\n",
    "plt.legend()\n",
    "plt.title('ROC-curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC and accuracy\n",
    "auc_train_RF = metrics.roc_auc_score(train_y, score_train_RF)\n",
    "accuractrain_y_RF = metrics.accuracy_score(train_y, (score_train_RF > threshold_RF))\n",
    "\n",
    "auc_test_RF = metrics.roc_auc_score(test_y, score_test_RF)\n",
    "accuractest_y_RF = metrics.accuracy_score(test_y, (score_test_RF > threshold_RF))\n",
    "\n",
    "print('Training data:\\nAUC: {auc}\\tAccuracy: {acc}\\n'\n",
    "      .format(auc=auc_train_RF, acc=accuractrain_y_RF))\n",
    "\n",
    "print('Test data:\\nAUC: {auc}\\tAccuracy: {acc}'\n",
    "      .format(auc=auc_test_RF, acc=accuractest_y_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = metrics.confusion_matrix(test_y, (score_test_RF > threshold_RF))\n",
    "\n",
    "plot_confusion_matrix(cm, [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = pd.DataFrame(\n",
    "    data=[['GBM', auc_test_GBM, accuracy_test_GBM],\n",
    "          ['RF', auc_test_RF,accuracy_test_RF]],\n",
    "    columns=['model', 'AUC', 'accuracy']\n",
    ")\n",
    "\n",
    "df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_test_RF, tpr_test_RF, label='RF')\n",
    "plt.plot(fpr_test_GBM, tpr_test_GBM, label='GBM')\n",
    "plt.legend()\n",
    "plt.title('ROC-curve on test data for all models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_best_model = os.path.join(config.path_model, 'mod_opt_{}.pkl'.format(today))\n",
    "\n",
    "with open(output_file_best_model, 'wb') as f:\n",
    "    pickle.dump(mod_opt_GBM, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or rolling window, retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_3 = list()\n",
    "observations_3 = list()\n",
    "\n",
    "reg_3 = xgb.XGBClassifier(n_estimators=100, objective='reg:squarederror')\n",
    "\n",
    "n_records = len(train_X)\n",
    "n_iterations = round((n_records-(train_size+window_size))/window_size)\n",
    "\n",
    "for i in range(0, n_iterations):\n",
    "\n",
    "    y = i * window_size\n",
    "  \n",
    "    train_X_iter = train_X.loc[y: y + train_size] \n",
    "    train_y_1_iter = train_y_1.loc[y: y + train_size]\n",
    "    \n",
    "    eval_X_iter = train_X.loc[1 + y + train_size: y + train_size + window_size]\n",
    "    eval_y_1_iter = train_y_1.loc[1 + y + train_size : y + train_size + window_size]\n",
    "    \n",
    "    reg_3.fit(train_X_iter, train_y_1_iter.values.ravel(),\n",
    "            eval_metric='mae',\n",
    "            eval_set=[(train_X_iter, train_y_1_iter.values.ravel()), (eval_X_iter, eval_y_1_iter.values.ravel())],\n",
    "            early_stopping_rounds=20,\n",
    "            verbose=False) # fit \n",
    "       \n",
    "    fig, ax = plt.subplots(figsize=(8, 2))\n",
    "    plot_importance(reg_3, ax = ax, height=0.5,  max_num_features= 12)\n",
    "\n",
    "    pred = reg_3.predict(eval_X_iter)\n",
    "    predictions_3.append(pred)\n",
    "    obs = eval_y_1_iter\n",
    "    observations_3.append(obs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
