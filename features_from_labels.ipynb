{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omore\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "pd.options.display.max_columns = None\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from datetime import datetime\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.~lock.train.csv#',\n",
       " 'hydro_1day.csv',\n",
       " 'hydro_coord.csv',\n",
       " 'ice_saw.csv',\n",
       " 'meteo_1day.csv',\n",
       " 'meteo_1month.csv',\n",
       " 'meteo_3hours.csv',\n",
       " 'meteo_coord.csv',\n",
       " 'reference_horiz_visib.csv',\n",
       " 'reference_water_codes.csv',\n",
       " 'test.csv',\n",
       " 'train.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Путь к директории с данными\n",
    "\n",
    "data_dir = '../00_data/track_1/'\n",
    "sumbit_dir = '../02_submits/'\n",
    "preprocessed_dir = '../03_preprocessed_data/'\n",
    "\n",
    "# Метео\n",
    "mc = pd.read_csv(data_dir + 'meteo_coord.csv')\n",
    "mld = pd.read_csv(data_dir + 'meteo_1day.csv')\n",
    "mlm = pd.read_csv(data_dir + 'meteo_1month.csv')\n",
    "rhv = pd.read_csv(data_dir + 'reference_horiz_visib.csv')\n",
    "\n",
    "# Гидро\n",
    "hc = pd.read_csv(data_dir + 'hydro_coord.csv')\n",
    "hld = pd.read_csv(data_dir + 'hydro_1day.csv',\n",
    "                   parse_dates=['date'])\n",
    "rwc = pd.read_csv(data_dir + 'reference_water_codes.csv')\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим всё\n",
    "main_df = pd.read_csv(data_dir + 'train.csv')\n",
    "test = pd.read_csv(data_dir + 'test.csv')\n",
    "# соединим train и test, чтобы фичи генерировались и на тесте тоже\n",
    "main_df = pd.concat([main_df, test])\n",
    "main_df['is_test'] = main_df['ice_jam'].isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Затор в прошлом году в тот же день"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station_id</th>\n",
       "      <th>day</th>\n",
       "      <th>ice_jam</th>\n",
       "      <th>is_test_x</th>\n",
       "      <th>ice_jam_same_day_last_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  station_id  day  ice_jam  is_test_x  ice_jam_same_day_last_year\n",
       "0  2000        3019    1      0.0      False                         0.0\n",
       "1  2000        3019    2      0.0      False                         0.0\n",
       "2  2000        3019    3      0.0      False                         0.0\n",
       "3  2000        3019    4      0.0      False                         0.0\n",
       "4  2000        3019    5      0.0      False                         0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df['previous_year'] = main_df['year'] - 1\n",
    "X = main_df.merge(main_df, \n",
    "              how = 'left', \n",
    "              left_on = ['previous_year', 'day', 'station_id'],\n",
    "              right_on = ['year', 'day', 'station_id']\n",
    "             ).drop(['previous_year_x', 'previous_year_y', 'year_y', 'is_test_y'], axis =1\n",
    "                   ).rename(columns={'year_x':'year', \n",
    "                 'ice_jam_x':'ice_jam',\n",
    "                 'ice_jam_y':'ice_jam_same_day_last_year'              \n",
    "                              })\n",
    "main_df = main_df.drop('previous_year', axis = 1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Затор в позапрошлом году в тот же день"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station_id</th>\n",
       "      <th>day</th>\n",
       "      <th>ice_jam</th>\n",
       "      <th>is_test</th>\n",
       "      <th>ice_jam_same_day_last_year</th>\n",
       "      <th>ice_jam_same_day_prelast_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  station_id  day  ice_jam  is_test  ice_jam_same_day_last_year  \\\n",
       "0  2000        3019    1      0.0    False                         0.0   \n",
       "1  2000        3019    2      0.0    False                         0.0   \n",
       "2  2000        3019    3      0.0    False                         0.0   \n",
       "3  2000        3019    4      0.0    False                         0.0   \n",
       "4  2000        3019    5      0.0    False                         0.0   \n",
       "\n",
       "   ice_jam_same_day_prelast_year  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4                            NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['preprevious_year'] = X['year'] - 2\n",
    "X = X.merge(main_df.drop('is_test', axis = 1),\n",
    "              how = 'left', \n",
    "              left_on = ['preprevious_year', 'day', 'station_id'],\n",
    "              right_on = ['year', 'day', 'station_id']\n",
    "             ).drop(['preprevious_year', 'year_y'], axis =1\n",
    "                   ).rename(columns={'year_x':'year', \n",
    "                 'ice_jam_x':'ice_jam',\n",
    "                 'ice_jam_y':'ice_jam_same_day_prelast_year',\n",
    "                 'is_test_x':'is_test'\n",
    "                              })\n",
    "X.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Индикатор - есть ли данные по заторам за прошлый год"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['is_last_year_missing'] = X['ice_jam_same_day_last_year'].isnull()\n",
    "X['is_prelast_year_missing'] = X['ice_jam_same_day_prelast_year'].isnull()\n",
    "X = X.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количество заторов в прошлом году"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[['n_jams_last_year','n_jams_prelast_year']] = \\\n",
    "    X.groupby(['year', 'station_id'])[['ice_jam_same_day_last_year', 'ice_jam_same_day_prelast_year']]\\\n",
    "    .transform('sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Количество заторов в прошлом по всем годам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_jams_per_year = X.groupby(['year', 'station_id'])['ice_jam'].sum().reset_index().drop_duplicates()\n",
    "ice_jams_per_year['n_jams_past'] = ice_jams_per_year.sort_values(by = 'year').\\\n",
    "    groupby(['station_id'])['ice_jam'].cumsum()\n",
    "ice_jams_per_year['n_jams_past'] = ice_jams_per_year['n_jams_past'] - ice_jams_per_year['ice_jam']\n",
    "ice_jams_per_year = ice_jams_per_year.drop('ice_jam', axis = 1)\n",
    "X = X.merge(ice_jams_per_year, how = 'left', on = ['year', 'station_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_same_days_before: how many days are available in the past for a given date\n",
    "X['n_same_days_before'] = X[['year', 'day', 'station_id']] \\\n",
    "    .sort_values(by = ['year', 'day']) \\\n",
    "    .groupby(['station_id', 'day']) \\\n",
    "    .cumcount()\n",
    "X['n_ice_jams_same_days_before'] = X[['year', 'day', 'station_id', 'ice_jam']] \\\n",
    "    .sort_values(by = ['year', 'day']) \\\n",
    "    .groupby(['station_id', 'day']) \\\n",
    "    ['ice_jam']\\\n",
    "    .cumsum()\n",
    "X['n_ice_jams_same_days_before'] = X['n_ice_jams_same_days_before'] - X['ice_jam']\n",
    "X['n_ice_jams_same_days_before_ratio'] = X['n_ice_jams_same_days_before']/X['n_same_days_before']\n",
    "X = X.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение по годам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    35.000000\n",
       "mean      3.942857\n",
       "std       4.014260\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       3.000000\n",
       "75%       6.000000\n",
       "max      16.000000\n",
       "Name: ice_jam, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Суммарное количество заторов в году\n",
    "jams_by_year = main_df.groupby('year').sum()['ice_jam'].to_frame().reset_index()\n",
    "jams_by_year['ice_jam'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 3]    18\n",
       "(3, 5]      7\n",
       "(5, 8]      6\n",
       "(8, 16]     4\n",
       "Name: ice_jam_bins, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Разделим на бины по квартилям\n",
    "bins = [-1, 3, 5, 8, 16]\n",
    "jams_by_year['ice_jam_bins'] = pd.cut(jams_by_year['ice_jam'], bins)\n",
    "X_length = jams_by_year[['year', 'ice_jam']]\n",
    "y_length = jams_by_year['ice_jam_bins']\n",
    "y_length.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим годы на трейн и тест\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_length, \n",
    "                                                    y_length, \n",
    "                                                    test_size=0.3,  \n",
    "                                                    stratify=y_length, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature description\n",
    "\n",
    "**is_test**:  if is_test = 1 then the row belongs to the test dataset \\\n",
    "**ice_jam_same_day_last_year**: 1 if there were and ice jam on the same day last year \\\n",
    "**ice_jam_same_day_prelast_year**: 1 if there were and ice jam on the same day 2 years ago \\\n",
    "**is_last_year_missing**: 1 is the same day is missing last year \\\n",
    "**is_prelast_year_missing**: 1 is the same day is missing 2 years ago \\\n",
    "**n_jams_last_year**: number of jams last year at the station \\\n",
    "**n_jams_past**: number of jams in the past available data at the station \\\n",
    "**n_same_days_before**: number of same days available in the past \\\n",
    "**n_ice_jams_same_days_before**: number of ice jams ob the same day in the past \\\n",
    "**n_ice_jams_same_days_before_ratio**: n_ice_jams_same_days_before/n_ice_jams_same_days_before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соберем фичи из гидроданных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Мы не можем использовать данные из будущего: всё, что происходит после заторного периода, относится уже к следующему году.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station_id</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>date</th>\n",
       "      <th>stage_avg</th>\n",
       "      <th>stage_min</th>\n",
       "      <th>stage_max</th>\n",
       "      <th>temp</th>\n",
       "      <th>water_code</th>\n",
       "      <th>ice_thickness</th>\n",
       "      <th>snow_height</th>\n",
       "      <th>place</th>\n",
       "      <th>discharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  station_id  month  day       date  stage_avg  stage_min  stage_max  \\\n",
       "0  2000        3019      1    1 2000-01-01       74.0       74.0       74.0   \n",
       "\n",
       "   temp water_code  ice_thickness  snow_height  place  discharge  \n",
       "0   NaN         46            NaN          NaN    NaN        NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Внесем не темпоральные данные\n",
    "X = pd.merge(X, hc[['station_id', \n",
    "                                'distance_from_source', \n",
    "                                'drainage_area', \n",
    "                                'z_null']], on='station_id', how='left')\n",
    "\n",
    "# Возьмем также данные из ежедневных наблюдений\n",
    "hld = pd.read_csv(data_dir + 'hydro_1day.csv',\n",
    "                   parse_dates=['date'])\n",
    "hld.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим на трейн и тест исходя из target_year\n",
    "\n",
    "test = X[X.year.isin(X_test.year.to_list()) & (X['is_test'] == False)].reset_index(drop=True).dropna()\n",
    "train = X[X.year.isin(X_train.year.to_list())& (X['is_test'] == False)].reset_index(drop=True).dropna()\n",
    "\n",
    "\n",
    "# Поделим данные на предикторы и таргет\n",
    "\n",
    "X_train, y_train = train.drop(['ice_jam'], axis = 1), train.ice_jam\n",
    "X_test, y_test = test.drop(['ice_jam'], axis = 1), test.ice_jam\n",
    "unused_features = ['year', 'is_test']\n",
    "\n",
    "X_train.to_csv(preprocessed_dir + 'X_train.csv', index = False)\n",
    "X_test.to_csv(preprocessed_dir + 'X_test.csv', index = False)\n",
    "y_train.to_csv(preprocessed_dir + 'y_train.csv', index = False)\n",
    "y_test.to_csv(preprocessed_dir + 'y_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hld['station_id'] = hld.station_id.astype(int)\n",
    "mld['station_id'] = mld['station_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station_id</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>date</th>\n",
       "      <th>stage_avg</th>\n",
       "      <th>stage_min</th>\n",
       "      <th>stage_max</th>\n",
       "      <th>temp</th>\n",
       "      <th>water_code</th>\n",
       "      <th>ice_thickness</th>\n",
       "      <th>snow_height</th>\n",
       "      <th>place</th>\n",
       "      <th>discharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>67.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>3019</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  station_id  month  day       date  stage_avg  stage_min  stage_max  \\\n",
       "0  2000        3019      1    1 2000-01-01       74.0       74.0       74.0   \n",
       "1  2000        3019      1    2 2000-01-02       70.0       70.0       70.0   \n",
       "2  2000        3019      1    3 2000-01-03       67.0       67.0       67.0   \n",
       "3  2000        3019      1    4 2000-01-04       64.0       64.0       64.0   \n",
       "4  2000        3019      1    5 2000-01-05       60.0       60.0       60.0   \n",
       "\n",
       "   temp water_code  ice_thickness  snow_height  place  discharge  \n",
       "0   NaN         46            NaN          NaN    NaN        NaN  \n",
       "1   NaN         46            NaN          NaN    NaN        NaN  \n",
       "2   NaN         46            NaN          NaN    NaN        NaN  \n",
       "3   NaN         46            NaN          NaN    NaN        NaN  \n",
       "4   NaN         46            NaN          NaN    NaN        NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hld.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>route_type</th>\n",
       "      <th>snow_coverage_near_station</th>\n",
       "      <th>snow_coverage_route</th>\n",
       "      <th>ice_crust_route</th>\n",
       "      <th>snow_height_aver</th>\n",
       "      <th>snow_height_max</th>\n",
       "      <th>snow_height_min</th>\n",
       "      <th>snow_density_aver</th>\n",
       "      <th>ice_crust_aver</th>\n",
       "      <th>snow_saturated_thickness</th>\n",
       "      <th>water_thickness</th>\n",
       "      <th>water_in_snow</th>\n",
       "      <th>water_total</th>\n",
       "      <th>snow_coverage_charact</th>\n",
       "      <th>snow_charact</th>\n",
       "      <th>snow_height</th>\n",
       "      <th>snow_coverage_station</th>\n",
       "      <th>snow_height_q1</th>\n",
       "      <th>snow_height_q2</th>\n",
       "      <th>snow_height_q3</th>\n",
       "      <th>temperature_20cm</th>\n",
       "      <th>temperature_20cm_qual</th>\n",
       "      <th>temperature_40cm</th>\n",
       "      <th>temperature_40cm_qual</th>\n",
       "      <th>temperature_80cm</th>\n",
       "      <th>temperature_80cm_qual</th>\n",
       "      <th>temperature_120cm</th>\n",
       "      <th>temperature_120cm_qual</th>\n",
       "      <th>temperature_160cm</th>\n",
       "      <th>temperature_160cm_qual</th>\n",
       "      <th>temperature_240cm</th>\n",
       "      <th>temperature_240cm_qual</th>\n",
       "      <th>temperature_320cm</th>\n",
       "      <th>temperature_320cm_qual</th>\n",
       "      <th>temperature_ks_5cm</th>\n",
       "      <th>temperature_ks_5cm_qual</th>\n",
       "      <th>temperature_ks_10cm</th>\n",
       "      <th>temperature_ks_10cm_qual</th>\n",
       "      <th>temperature_ks_15cm</th>\n",
       "      <th>temperature_ks_15cm_qual</th>\n",
       "      <th>temperature_ks_20cm</th>\n",
       "      <th>temperature_ks_20cm_qual</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24538</td>\n",
       "      <td>1985</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1985-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24538</td>\n",
       "      <td>1985</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1985-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24538</td>\n",
       "      <td>1985</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1985-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24538</td>\n",
       "      <td>1985</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1985-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24538</td>\n",
       "      <td>1985</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1985-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id  year  month  day  route_type  snow_coverage_near_station  \\\n",
       "0       24538  1985      1    1         NaN                         NaN   \n",
       "1       24538  1985      1    2         NaN                         NaN   \n",
       "2       24538  1985      1    3         NaN                         NaN   \n",
       "3       24538  1985      1    4         NaN                         NaN   \n",
       "4       24538  1985      1    5         NaN                         NaN   \n",
       "\n",
       "   snow_coverage_route  ice_crust_route  snow_height_aver  snow_height_max  \\\n",
       "0                  NaN              NaN               NaN              NaN   \n",
       "1                  NaN              NaN               NaN              NaN   \n",
       "2                  NaN              NaN               NaN              NaN   \n",
       "3                  NaN              NaN               NaN              NaN   \n",
       "4                  NaN              NaN               NaN              NaN   \n",
       "\n",
       "   snow_height_min  snow_density_aver  ice_crust_aver  \\\n",
       "0              NaN                NaN             NaN   \n",
       "1              NaN                NaN             NaN   \n",
       "2              NaN                NaN             NaN   \n",
       "3              NaN                NaN             NaN   \n",
       "4              NaN                NaN             NaN   \n",
       "\n",
       "   snow_saturated_thickness  water_thickness  water_in_snow  water_total  \\\n",
       "0                       NaN              NaN            NaN          NaN   \n",
       "1                       NaN              NaN            NaN          NaN   \n",
       "2                       NaN              NaN            NaN          NaN   \n",
       "3                       NaN              NaN            NaN          NaN   \n",
       "4                       NaN              NaN            NaN          NaN   \n",
       "\n",
       "   snow_coverage_charact  snow_charact  snow_height  snow_coverage_station  \\\n",
       "0                    NaN           NaN         32.0                   10.0   \n",
       "1                    NaN           NaN         32.0                   10.0   \n",
       "2                    NaN           NaN         32.0                   10.0   \n",
       "3                    NaN           NaN         32.0                   10.0   \n",
       "4                    NaN           NaN         32.0                   10.0   \n",
       "\n",
       "   snow_height_q1  snow_height_q2  snow_height_q3  temperature_20cm  \\\n",
       "0             0.0             0.0             0.0               NaN   \n",
       "1             0.0             0.0             0.0               NaN   \n",
       "2             0.0             0.0             0.0               NaN   \n",
       "3             0.0             0.0             0.0               NaN   \n",
       "4             0.0             0.0             0.0               NaN   \n",
       "\n",
       "   temperature_20cm_qual  temperature_40cm  temperature_40cm_qual  \\\n",
       "0                    NaN               NaN                    NaN   \n",
       "1                    NaN               NaN                    NaN   \n",
       "2                    NaN               NaN                    NaN   \n",
       "3                    NaN               NaN                    NaN   \n",
       "4                    NaN               NaN                    NaN   \n",
       "\n",
       "   temperature_80cm  temperature_80cm_qual  temperature_120cm  \\\n",
       "0               NaN                    NaN                NaN   \n",
       "1               NaN                    NaN                NaN   \n",
       "2               NaN                    NaN                NaN   \n",
       "3               NaN                    NaN                NaN   \n",
       "4               NaN                    NaN                NaN   \n",
       "\n",
       "   temperature_120cm_qual  temperature_160cm  temperature_160cm_qual  \\\n",
       "0                     NaN                NaN                     NaN   \n",
       "1                     NaN                NaN                     NaN   \n",
       "2                     NaN                NaN                     NaN   \n",
       "3                     NaN                NaN                     NaN   \n",
       "4                     NaN                NaN                     NaN   \n",
       "\n",
       "   temperature_240cm  temperature_240cm_qual  temperature_320cm  \\\n",
       "0                NaN                     NaN                NaN   \n",
       "1                NaN                     NaN                NaN   \n",
       "2                NaN                     NaN                NaN   \n",
       "3                NaN                     NaN                NaN   \n",
       "4                NaN                     NaN                NaN   \n",
       "\n",
       "   temperature_320cm_qual  temperature_ks_5cm  temperature_ks_5cm_qual  \\\n",
       "0                     NaN                 NaN                      NaN   \n",
       "1                     NaN                 NaN                      NaN   \n",
       "2                     NaN                 NaN                      NaN   \n",
       "3                     NaN                 NaN                      NaN   \n",
       "4                     NaN                 NaN                      NaN   \n",
       "\n",
       "   temperature_ks_10cm  temperature_ks_10cm_qual  temperature_ks_15cm  \\\n",
       "0                  NaN                       NaN                  NaN   \n",
       "1                  NaN                       NaN                  NaN   \n",
       "2                  NaN                       NaN                  NaN   \n",
       "3                  NaN                       NaN                  NaN   \n",
       "4                  NaN                       NaN                  NaN   \n",
       "\n",
       "   temperature_ks_15cm_qual  temperature_ks_20cm  temperature_ks_20cm_qual  \\\n",
       "0                       NaN                  NaN                       NaN   \n",
       "1                       NaN                  NaN                       NaN   \n",
       "2                       NaN                  NaN                       NaN   \n",
       "3                       NaN                  NaN                       NaN   \n",
       "4                       NaN                  NaN                       NaN   \n",
       "\n",
       "         date  \n",
       "0  1985-01-01  \n",
       "1  1985-01-02  \n",
       "2  1985-01-03  \n",
       "3  1985-01-04  \n",
       "4  1985-01-05  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mld.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station_id</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>date_x</th>\n",
       "      <th>stage_avg</th>\n",
       "      <th>stage_min</th>\n",
       "      <th>stage_max</th>\n",
       "      <th>temp</th>\n",
       "      <th>water_code</th>\n",
       "      <th>ice_thickness</th>\n",
       "      <th>snow_height_x</th>\n",
       "      <th>place</th>\n",
       "      <th>discharge</th>\n",
       "      <th>route_type</th>\n",
       "      <th>snow_coverage_near_station</th>\n",
       "      <th>snow_coverage_route</th>\n",
       "      <th>ice_crust_route</th>\n",
       "      <th>snow_height_aver</th>\n",
       "      <th>snow_height_max</th>\n",
       "      <th>snow_height_min</th>\n",
       "      <th>snow_density_aver</th>\n",
       "      <th>ice_crust_aver</th>\n",
       "      <th>snow_saturated_thickness</th>\n",
       "      <th>water_thickness</th>\n",
       "      <th>water_in_snow</th>\n",
       "      <th>water_total</th>\n",
       "      <th>snow_coverage_charact</th>\n",
       "      <th>snow_charact</th>\n",
       "      <th>snow_height_y</th>\n",
       "      <th>snow_coverage_station</th>\n",
       "      <th>snow_height_q1</th>\n",
       "      <th>snow_height_q2</th>\n",
       "      <th>snow_height_q3</th>\n",
       "      <th>temperature_20cm</th>\n",
       "      <th>temperature_20cm_qual</th>\n",
       "      <th>temperature_40cm</th>\n",
       "      <th>temperature_40cm_qual</th>\n",
       "      <th>temperature_80cm</th>\n",
       "      <th>temperature_80cm_qual</th>\n",
       "      <th>temperature_120cm</th>\n",
       "      <th>temperature_120cm_qual</th>\n",
       "      <th>temperature_160cm</th>\n",
       "      <th>temperature_160cm_qual</th>\n",
       "      <th>temperature_240cm</th>\n",
       "      <th>temperature_240cm_qual</th>\n",
       "      <th>temperature_320cm</th>\n",
       "      <th>temperature_320cm_qual</th>\n",
       "      <th>temperature_ks_5cm</th>\n",
       "      <th>temperature_ks_5cm_qual</th>\n",
       "      <th>temperature_ks_10cm</th>\n",
       "      <th>temperature_ks_10cm_qual</th>\n",
       "      <th>temperature_ks_15cm</th>\n",
       "      <th>temperature_ks_15cm_qual</th>\n",
       "      <th>temperature_ks_20cm</th>\n",
       "      <th>temperature_ks_20cm_qual</th>\n",
       "      <th>date_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [year, station_id, month, day, date_x, stage_avg, stage_min, stage_max, temp, water_code, ice_thickness, snow_height_x, place, discharge, route_type, snow_coverage_near_station, snow_coverage_route, ice_crust_route, snow_height_aver, snow_height_max, snow_height_min, snow_density_aver, ice_crust_aver, snow_saturated_thickness, water_thickness, water_in_snow, water_total, snow_coverage_charact, snow_charact, snow_height_y, snow_coverage_station, snow_height_q1, snow_height_q2, snow_height_q3, temperature_20cm, temperature_20cm_qual, temperature_40cm, temperature_40cm_qual, temperature_80cm, temperature_80cm_qual, temperature_120cm, temperature_120cm_qual, temperature_160cm, temperature_160cm_qual, temperature_240cm, temperature_240cm_qual, temperature_320cm, temperature_320cm_qual, temperature_ks_5cm, temperature_ks_5cm_qual, temperature_ks_10cm, temperature_ks_10cm_qual, temperature_ks_15cm, temperature_ks_15cm_qual, temperature_ks_20cm, temperature_ks_20cm_qual, date_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hld = hld.merge(mld, on=['station_id', 'year', 'month', 'day'], how='inner')\n",
    "hld = hld.fillna(0)\n",
    "hld.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скорректируем год, в который доступно наблюдение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hld.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим для наблюдения год, в который оно доступно\n",
    "\n",
    "# Маска-окно между заторным периодом и концом года\n",
    "# true if after jam\n",
    "# false otherwise\n",
    "def after_jam_window(row, local=False):\n",
    "    if local:\n",
    "        month = row.month_local\n",
    "        day = row.date_local.day\n",
    "    else:\n",
    "        month = row.month\n",
    "        day = row.date.day\n",
    "    return (((month == 6) and (day > 3))\n",
    "            or (month in [7, 8, 9, 10, 11, 12]))\n",
    "\n",
    "# Год относительно бизнес-логики\n",
    "def target_year(row, local=False):\n",
    "    if local:\n",
    "        year = row.year_local\n",
    "    else:\n",
    "        year = row.year\n",
    "    if after_jam_window(row):\n",
    "        return year + 1\n",
    "    else:\n",
    "        return year\n",
    "    \n",
    "hld['target_year'] = hld.apply(target_year, axis=1)\n",
    "\n",
    "# Календарный год и день больше не нужны\n",
    "hld.drop(columns=['year', 'date', 'day'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hld.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гидро – сделаем ресэмплинг по месяцу\n",
    "\n",
    "index = ['station_id', 'month', 'target_year']\n",
    "\n",
    "hld_mean = hld.groupby(index).mean().add_prefix('mean_').reset_index()\n",
    "hld_max = hld.groupby(index).max().add_prefix('max_').reset_index()\n",
    "hld_min = hld.groupby(index).min().add_prefix('min_').reset_index()\n",
    "hld_std = hld.groupby(index).std().add_prefix('std_').reset_index()\n",
    "data_frames = [hld_mean, hld_max, hld_min, hld_std]\n",
    "\n",
    "hydro_monthly = pd.concat(data_frames, axis=1)\n",
    "hydro_monthly = hydro_monthly.loc[:,~hydro_monthly.columns.duplicated()]\n",
    "hydro_monthly.sort_values(index).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df):\n",
    "    station, target_year = df.name\n",
    "    result = pd.DataFrame()\n",
    "    for month, mdf in df.groupby('month'):\n",
    "        m_feats = mdf[df.columns[4:]].add_prefix(str(month) + '_').reset_index(drop=True)\n",
    "        result = pd.concat([result, m_feats], axis=1)\n",
    "    return result.reset_index(drop=True)\n",
    "        \n",
    "hydro_features = hydro_monthly.groupby(['station_id', 'target_year']).apply(make_features)\n",
    "hydro_features = hydro_features.reset_index(level=2, drop=True).reset_index()\n",
    "hydro_features.dropna(how='all', axis=1, inplace=True)\n",
    "hydro_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соберем фичи в основной датасет\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Важно: merge делаем по target_year.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.merge(main_df, hydro_features, left_on=['year', 'station_id'],\n",
    "                   right_on=['target_year', 'station_id'],\n",
    "                   how='left')\n",
    "cols = main_df.columns.to_list()\n",
    "main_df = main_df[cols[:3] + [cols[7]] + cols[5:7] + cols[8:] + [cols[3]]]\n",
    "main_df.dropna(how='any',inplace=True)\n",
    "main_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормируем фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ids, data, target = main_df[main_df.columns[:4]], main_df[main_df.columns[4:-1]], main_df[main_df.columns[-1]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "transformed_data = scaler.transform(data)\n",
    "norm_df = pd.concat([ids, pd.DataFrame(transformed_data, columns = main_df.columns[4:-1]), target], axis=1)\n",
    "norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим на трейн и тест исходя из target_year\n",
    "\n",
    "test = norm_df[~norm_df.target_year.isin(X_train.year.to_list())].reset_index(drop=True).dropna()\n",
    "train = norm_df[norm_df.target_year.isin(X_train.year.to_list())].reset_index(drop=True).dropna()\n",
    "\n",
    "# target_year больше не нужна\n",
    "\n",
    "test.drop(columns=['target_year'], inplace=True)\n",
    "train.drop(columns=['target_year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поделим данные на предикторы и таргет\n",
    "\n",
    "X_train, y_train = train.iloc[:, :-1], train.ice_jam\n",
    "X_test, y_test = test.iloc[:, :-1], test.ice_jam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим простой классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#lsvc = SVC(C=0.01, penalty=\"l1\", dual=False).fit(X_train, y_train)\n",
    "selector = RFE(LogisticRegression(max_iter=5000),\n",
    "               n_features_to_select=200, step=10).fit(X_train, y_train)\n",
    "X_reduced = selector.transform(X_train)\n",
    "X_reduced_test = selector.transform(X_test)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "parameters = {'pca__n_components': list(range(10,50,5)), 'svc__kernel':('linear', 'rbf'), 'svc__C':[1,10]}\n",
    "clf = Pipeline([('pca', pca), ('svc', SVC(class_weight='balanced'))])\n",
    "GS = GridSearchCV(clf, parameters, scoring='f1_macro')\n",
    "GS.fit(X_train, y_train)\n",
    "print(GS.cv_results_)\n",
    "print(GS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = GS.predict(X_test)\n",
    "recall_score(y_test, prediction), \\\n",
    "    precision_score(y_test, prediction), f1_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(data_dir + 'test.csv')\n",
    "test_real = pd.read_csv(data_dir + '../test_SECRET.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.merge(test_real, hc[['station_id', \n",
    "                                'distance_from_source', \n",
    "                                'drainage_area', \n",
    "                                'z_null']], on='station_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'year' in hydro_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.merge(test_X, hydro_features, left_on=['year', 'station_id'],\n",
    "                   right_on=['target_year', 'station_id'],\n",
    "                   how='left')\n",
    "cols = test_X.columns.to_list()\n",
    "test_X = test_X[cols[:3] + [cols[7]] + cols[5:7] + cols[8:] + [cols[3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, data, target = test_X[test_X.columns[:4]], test_X[test_X.columns[4:-1]], test_X[test_X.columns[-1]]\n",
    "\n",
    "transformed_data = scaler.transform(data)\n",
    "test_X = pd.concat([ids, pd.DataFrame(transformed_data, columns = main_df.columns[4:-1]), target], axis=1)\n",
    "X_test_real, y_test_real = test_X.iloc[:, :-1], test_X.ice_jam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_real.drop('target_year',inplace=True,axis=1)\n",
    "X_test_real = X_test_real.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_real_reduced = selector.transform(X_test_real)\n",
    "prediction =  GS.predict(X_test_real)\n",
    "accuracy_score(y_test_real, prediction), recall_score(y_test_real, prediction), \\\n",
    "    precision_score(y_test_real, prediction), f1_score(y_test_real, prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
